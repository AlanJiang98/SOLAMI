NAME: Instruct_HumanML3D-20240507-71515 # Experiment names
ACCELERATOR: 'gpu' # Devices optioncal: “cpu”, “gpu”, “tpu”, “ipu”, “hpu”, “mps, “auto”
NUM_NODES: 1 # Number of GPU nodes for distributed training
DEVICE: [0, 1, 2, 3] # Index of gpus eg. [0] or [0,1,2,3]

TRAIN:
  #---------------------------------
  STAGE: lm_instruct # stage "vae" , "lm_pretrain", "lm_instruct"
  #---------------------------------
  NUM_WORKERS: 8 # Number of workers
  BATCH_SIZE: 24 # Size of batches
  END_EPOCH: 350 # End epoch
  RESUME: '' # Resume training from this path
  PRETRAINED: 'experiments/mgpt/Pretrain_HumanML3D_GPT2_SOTA_0507/checkpoints/epoch=164.ckpt' #'experiments/mgpt/Pretrain_HumanML3D_GPT2/checkpoints/epoch=99-v1.ckpt' # Preatrained model path
  PRETRAINED_VAE: '' #'experiments/mgpt/VQVAE_HumanML3D_3000epoch512/checkpoints/epoch=2999-v1.ckpt' # checkpoints/MotionGPT-base/motiongpt_s3_h3d.tar # Vae model path
  TASK: 'interaction'
  SPEECH_TOKENS: 'special'
  RIGHT_PADDING: False
  ONLY_ANSWER: True

  OPTIM:
    target: AdamW
    params:
      lr: 1e-4
      betas: [0.9, 0.99]
      weight_decay: 0.0

# Evaluating Configuration
EVAL:
  BATCH_SIZE: 64 # Evaluating Batch size
  SPLIT: test

TEST:
  CHECKPOINTS: experiments/mgpt/Pretrain_HumanML3D_GPT2_SOTA_0507/checkpoints/epoch=164.ckpt #experiments/mgpt/Instruct_HumanML3D-20240418-GPT2-Inter-V01/checkpoints/epoch=3-v1.ckpt
  # CHECKPOINTS: experiments/mgpt/Instruct_HumanML3D-20240320/checkpoints/last.ckpt
  SPLIT: test
  BATCH_SIZE: 64 # training Batch size

DATASET:
  target: mGPT.data.HumanML3D.HumanML3DDataModule
  CODE_PATH: TOKENS_SOTA

METRIC:
  TYPE: ["TM2TMetrics"] #['PredMetrics', 'TM2TMetrics']

LOSS:
  LAMBDA_FEATURE: 1.0
  LAMBDA_VELOCITY: 0.5
  LAMBDA_COMMIT: 0.02
  LAMBDA_CLS: 1.0
  ABLATION:
    RECONS_LOSS: 'l1_smooth'

model:
  target: mGPT.models.mgpt.MotionGPT
  params:
    condition: 'text'
    task: 'interaction'
    lm: ${lm.gpt2_medium}
    motion_vae: ${vq.default}

LOGGER:
  TYPE: ['tensorboard', 'wandb']
  VAL_EVERY_STEPS: 1
  WANDB:
    params:
      project: mem